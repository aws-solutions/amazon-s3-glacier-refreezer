# Amazon S3 Glacier Re:Freezer 
The Amazon S3 Glacier Re:Freezer is a serverless solution that seamlessly copies all 
Amazon S3 Glacier Vault’s archives to a selected Amazon S3 bucket.

Customers have used Amazon S3 Glacier based Vaults as a cost effective and durable
target to store their long term archive data. The release of Amazon S3 Glacier Deep
Archive as the lowest cost Amazon S3 storage class raised interest from the existing
Amazon S3 Glacier Vault customers who wanted to save further costs on storage.
However there was no simple and seamless solution to enable them to copy their data from
their Amazon S3 Glacier Vaults to the Amazon S3 Glacier Deep Archive storage class.

The Amazon S3 Glacier Re:Freezer solution was built to directly address this customer
requirement.

With configurable target storage class customers can select Amazon S3 Deep 
Archive for more cost efficient storage option or Amazon S3 Standard in the situations where 
all-at-once backup restore is required.

## Table of contents
- [Architecture](#architecture)
- [Project structure](#project-structure)
- [Deployment](#deployment)
- [Runtime Monitoring](#monitoring)
- [Creating a custom build](#creating-a-custom-build)
- [Additional Resources](#additional-resources)

## Architecture

Architecture:

![Amazon S3 Glacier Re:Freezer](source/images/amazon-s3-glacier-refreezer-architecture.png)

The solution utilizes the following as inputs for deployment: 
- Source Amazon S3 Glacier Vault
- Amazon S3 Glacier Retrieval Tier
- Target Amazon S3 Bucket
- Target Amazon S3 Storage Class

### Stage One: Get Inventory

CloudFormation deployment process starts the solution automatically by triggering Request Inventory Lambda function that will check the input parameters and make sure that destination bucket and the glacier vault are accessible. Then it will request the fresh Glacier Vault inventory report as the source list of the Glacier Vault archives to be copied to the destination Amazon S3 bucket and storage class. 

**OBS! Inventory generation can take up to 4 hours.**

Once inventory has been generated by Glacier and the notification sent to the Inventory Notification Topic, a set of Lambda functions will download the latest inventory into the staging bucket and trigger Stage Two.

### Stage Two: Request Archives Restoration
In Stage Two the solution will then parse and partition the Glacier Vault inventory list to optimize the restore process. Once optimisation has been complete, Request Archives lambda function will be triggered processing 10,000 archives per invocation - one lambda call per partition. The function will be triggered  multiple times sequentially, until all archives have been requested.

**OBS! Retrieval can take up to 8 hours for Standard Tier and 12 hours for Bulk.**

### Stage Three: Copy Archives

Once a Glacier Vault archive has been retrieved and is available for the download, the solution initiates the archive copy process to the staging S3 bucket's S3-Standard storage class.

If the archive is larger than 4GB, the solution calculates the number of the 4 GB chunks, opens a multipart upload request and submits each chunk as a separate request into a download-chunk-queue. Another asynchrnous lambda function will read from the queue, download each chunk, and, if the chunk has identified to be the last one through the DynamoDB status table, close off the multi part upload.

### Stage Four: Validation and Final Move

On completion of the copy process, the solution calculates a SHA256 Treehash of the copied object, and matches it to the SHA256 Treehash as recorded by Glacier in the Glacier Vault inventory list. Once the SHA256 Treehash has been validated, the object is finally moved from the staging bucket to the destination bucket and the destination storage class. The final move consistency relies on the validation check performed by the Amazon S3 Service.

### State management and Progress Tracking

During the copy operation process, Amazon DynamoDB is used to keep track of the status of the archive copies, where the copy operation progress is visibile through the provided Amazon CloudWatch dashboard.

## Deployment

> **Please ensure you test the solutions prior running it against any production vaults.**

The solution supports two deployment options:

### Option 1. Solution Builders Template

You can launch this solution with one click from the [solution home page](https://aws.amazon.com/solutions/implementations/amazon-s3-glacier-refreezer).

### Option 2. Custom Build

Please refer to [creating a custom build](#creating-a-custom-build) section below.

### Deployment Configuration 

The solution requires the following input parameters to be provided at CloudFromation stack deployment:

- Source Amazon S3 Glacier Vault
- Amazon S3 Glacier Retrieval Tier
- Target Amazon S3 Destination Bucket (should be pre-created before deployment)
- Target Amazon S3 Storage Class

Cost acknowledgements:
- Vault default SNS Topic confirmation <br>
  To confirm that there is no default notification topic configured. Or if there is one, it is OK to proceed with every archive retrieval API call resulting in an SNS notification.

- CloudTrail configuration confirmation <br>
  To confirm that there is only a single cloud trail export configured against the AWS account containing the source Glacier Vault. Glacier API calls are classified as management API calls and incur CloudTrail charges for each additional CloudTrail export configured in addition to the first one (which is free).

[Optional]:
- ArchiveDescription override filename location in the format: mybucket/myfile.csv
  The location of the file that provides ArchiveDescription override for the Vault archives. 

### Progress Tracking

Once deployed, the CloudFromation Output tab will have the link to Amazon CloudWatch progress dashboard - <STACK_NAME>-Amazon-S3-Glacier-ReFreezer.

![Amazon S3 Glacier Re:Freezer Progress Metrics](source/images/dashboard.png)

### Anonymous Statistics Collection

The deployment will collect and send anonymously to the AWS Solution Builders team the following data points: 

- Region 
- Target Storage Class
- Vault Archive Count
- Vault Size

## Project structure

```
├── deployment
│   └── cdk-solution-helper  [Lightweight helper that cleans-up synthesized templates from the CDK and removes Standard and Expedited options]
├── source
│   ├── bin                  [Entrypoint of the CDK application]
│   ├── images               [Assets referenced in the README file]
│   ├── lambda               [Lambda processing logic]
│   ├── lib                  [Constructs for the components of the solution]
│   └── test                 [Unit tests]
```

## Creating a custom build

To customize and deploy the solution from the source code, follow the steps below:

### Prerequisites

Install prerequisite software packages:

* [AWS Command Line Interface](https://aws.amazon.com/cli/)
* [AWS Cloud Development Kit (AWS CDK)](https://aws.amazon.com/cdk/)
* [Node package manager](https://www.npmjs.com/)
* Node.js 12.x or later

### 1. Download or clone this repo
```
git clone https://github.com/awslabs/amazon-s3-glacier-refreezer
```

### 2. Download NPM packages required for Lambda functions
```
cd ./source
chmod +x ./*.sh
./install-node-modules.sh
```

### 3. After introducing changes, run the unit tests to make sure the customizations don't break existing functionality

```
./run-all-tests.sh
```

### 4. Deploy the solution using CDK

Make sure AWS CLI is operational:

```
aws s3 ls
```

Bootstrap CDK, if required

```
cdk bootstrap
```

Deploy the solution.

Set environment variable GRF_STACK_NAME with the name of cloudformation stack. Replace my-stack-1 with a name of your choice.

```
export GRF_STACK_NAME=<<my-stack-1>>
```

Replace the parameter placeholders designated as <<>>. Provide name of the source glacier vault, name of the destination S3 bucket, the destination storage class (e.g. DEEP_ARCHIVE, STANDARD, etc.) and the optional parameter - Amazon S3 location where you have stored the mapping file.

```
cdk deploy  --parameters SourceVault=<<my-source-vault>>  \
            --parameters DestinationBucket=<<my-destination-bucket>> \
            --parameters DestinationStorageClass=DEEP_ARCHIVE \
            --parameters GlacierRetrievalTier=Bulk \
            --parameters CloudTrailExportConfirmation=Yes \
            --parameters SNSTopicForVaultConfirmation=Yes \
            --parameters FilelistS3Location=<<my-filelist-override-bucket/my-override-filename.csv>>
```

Leave FilelistS3Location (optional parameter) empty if you are not providing a mapping file. 

```
            --parameters FilelistS3Location=
```

### 5. Prepare Cloudformation Template

The solution expects the CloudFromation template to be uploaded into Amazon S3 Bucket named as:

    <BUCKET_BASE>-<REGION>

For example:

    my-glacier-refreezer-ap-southeast-2

To prepare the template:
```
cd ./deployment
chmod +x ./*.sh

BUCKET_BASE=my-glacier-refreezer   # S3 bucket name BASE 
                                   # i.e. without the region designator 
SOLUTION_NAME=my-solution-name     # custom solution name
VERSION=my-version                 # custom version number

./build-s3-dist.sh $BUCKET_BASE $SOLUTION_NAME $VERSION
```

### 6. Upload deployment assets to your Amazon S3 buckets

Create the CloudFormation bucket as defined above in the region you wish to deploy. 

The CloudFormation templates are configured to pull the Lambda deployment packages from Amazon S3 bucket in the region the template is being launched in.

```
aws s3 mb s3://$BUCKET_BASE-ap-southeast-2 --region ap-southeast-2
```

To upload the template to the regional bucket:

```
BUCKET_NAME=my-glacier-refreezer-ap-southeast-2    # full regional bucket name
SOLUTION_NAME=my-solution-name                     # custom solution name
VERSION=my-version                                 # custom version number

aws s3 cp ./global-s3-assets/   s3://${BUCKET_NAME}/${SOLUTION}/${VERSION} --recursive --acl public-read --acl bucket-owner-full-control
aws s3 cp ./regional-s3-assets/ s3://${BUCKET_NAME}/${SOLUTION}/${VERSION} --recursive --acl public-read --acl bucket-owner-full-control 

echo "https://${BUCKET_NAME}.s3.amazonaws.com/${SOLUTION_NAME}/${VERSION}/${SOLUTION_NAME}.template"
```

### 7. Launch the CloudFormation template

* Get the link of the template uploaded to your Amazon S3 bucket (created as $BUCKET_NAME in the previous step)
* Deploy the solution to your account by launching a new AWS CloudFormation stack

## Additional Resources

### Services
- [AWS DynamoDB](https://aws.amazon.com/dynamodb/)
- [AWS Lambda](https://aws.amazon.com/lambda/)
- [AWS Glue](https://aws.amazon.com/glue/)
- [Amazon Athena](https://aws.amazon.com/athena/)
- [Amazon S3](https://aws.amazon.com/s3/)
- [Amazon S3 Glacier](https://aws.amazon.com/glacier/)

***

Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
